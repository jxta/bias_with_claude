#!/usr/bin/env sage

"""
è¶…å¤§è¦æ¨¡å®Ÿé¨“ã‚·ã‚¹ãƒ†ãƒ  (10^9è¦æ¨¡å¯¾å¿œ)
åˆ†æ•£å‡¦ç†ãƒ»ãƒ¡ãƒ¢ãƒªæœ€é©åŒ–ãƒ»ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼å¯¾å¿œ

ç‰¹å¾´:
- 10^9è¦æ¨¡ (50,847,534ç´ æ•°) ã®è¶…å¤§è¦æ¨¡å®Ÿé¨“å¯¾å¿œ
- åˆ†æ•£å‡¦ç†ã«ã‚ˆã‚‹ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼è¨ˆç®—
- ãƒ¡ãƒ¢ãƒªåŠ¹ç‡çš„ãªã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°å‡¦ç†
- éšœå®³è€æ€§ã¨ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆæ©Ÿèƒ½
- è‡ªå‹•ãƒªã‚½ãƒ¼ã‚¹ç®¡ç†ã¨ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°
- ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ç›£è¦–ã¨ã‚¢ãƒ©ãƒ¼ãƒˆ

ä½œæˆè€…: Claude & é’æœ¨ç¾ç©‚ç ”ç©¶ã‚°ãƒ«ãƒ¼ãƒ—
æ—¥ä»˜: 2025/07/16
"""

import json
import os
import time
import pickle
import multiprocessing as mp
from multiprocessing import Pool, Manager, Value, Lock, Queue
from concurrent.futures import ProcessPoolExecutor, ThreadPoolExecutor, as_completed
import psutil
import signal
import sys
from datetime import datetime
import numpy as np
from tqdm import tqdm
import gc
import threading
import queue
import mmap
import tempfile
import shutil
import logging
from pathlib import Path
import socket
import uuid
import sqlite3
from collections import defaultdict
import math

# SageMathç’°å¢ƒã®ç¢ºèª
try:
    from sage.all import *
    SAGE_ENV = True
except ImportError:
    print("âš ï¸  SageMathç’°å¢ƒãŒå¿…è¦ã§ã™")
    SAGE_ENV = False

# Omarè«–æ–‡ã®13ã‚±ãƒ¼ã‚¹ï¼ˆlarge_scale_experiment.pyã‹ã‚‰ç¶™æ‰¿ï¼‰
from .large_scale_experiment import OMAR_CASES

class UltraLargeScaleManager:
    """è¶…å¤§è¦æ¨¡å®Ÿé¨“ç®¡ç†ã‚·ã‚¹ãƒ†ãƒ """
    
    def __init__(self, x_max=1000000000, num_workers=None, chunk_size=50000, 
                 distributed=False, nodes=None):
        self.x_max = x_max
        self.num_workers = num_workers or min(mp.cpu_count(), 32)
        self.chunk_size = chunk_size
        self.distributed = distributed
        self.nodes = nodes or []
        self.output_dir = f"ultra_large_results_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
        
        # åˆ†æ•£å‡¦ç†è¨­å®š
        self.master_node = socket.gethostname()
        self.node_id = str(uuid.uuid4())[:8]
        
        # ãƒ¡ãƒ¢ãƒªåŠ¹ç‡åŒ–è¨­å®š
        self.memory_limit = psutil.virtual_memory().total * 0.8  # 80%åˆ¶é™
        self.temp_dir = tempfile.mkdtemp(prefix="ultra_large_")
        
        # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹è¨­å®š
        self.db_path = f"{self.output_dir}/experiment.db"
        self.init_database()
        
        # ãƒ­ã‚°è¨­å®š
        self.setup_logging()
        
        # çµ±è¨ˆæƒ…å ±
        self.stats = {
            'total_primes_target': self.estimate_prime_count(x_max),
            'chunks_processed': 0,
            'memory_peak': 0,
            'nodes_active': len(self.nodes) if distributed else 1,
            'start_time': None,
            'checkpoints_saved': 0
        }
        
        print(f"ğŸš€ è¶…å¤§è¦æ¨¡å®Ÿé¨“ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–å®Œäº†")
        print(f"ğŸ¯ è¦æ¨¡: {x_max:,} ({x_max/1000000000:.1f}B) ç´ æ•°ã¾ã§")
        print(f"ğŸ“Š æ¨å®šç´ æ•°æ•°: {self.stats['total_primes_target']:,}")
        print(f"ğŸ§® ä¸¦åˆ—åº¦: {self.num_workers}ã‚³ã‚¢")
        if distributed:
            print(f"ğŸŒ åˆ†æ•£å‡¦ç†: {len(self.nodes)} ãƒãƒ¼ãƒ‰")
        
    def estimate_prime_count(self, n):
        """ç´ æ•°å®šç†ã«ã‚ˆã‚‹ç´ æ•°æ•°ã®æ¨å®š"""
        if n < 2:
            return 0
        return int(n / (math.log(n) - 1))
    
    def init_database(self):
        """å®Ÿé¨“ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã®åˆæœŸåŒ–"""
        os.makedirs(self.output_dir, exist_ok=True)
        
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        # ãƒ†ãƒ¼ãƒ–ãƒ«ä½œæˆ
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS experiments (
                id INTEGER PRIMARY KEY,
                case_name TEXT,
                status TEXT,
                start_time TIMESTAMP,
                end_time TIMESTAMP,
                result_path TEXT
            )
        ''')
        
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS prime_results (
                id INTEGER PRIMARY KEY,
                case_name TEXT,
                prime INTEGER,
                frobenius_element TEXT,
                chunk_id INTEGER,
                node_id TEXT,
                timestamp TIMESTAMP
            )
        ''')
        
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS checkpoints (
                id INTEGER PRIMARY KEY,
                case_name TEXT,
                chunk_id INTEGER,
                progress REAL,
                data_path TEXT,
                timestamp TIMESTAMP
            )
        ''')
        
        conn.commit()
        conn.close()
    
    def setup_logging(self):
        """ãƒ­ã‚°ã‚·ã‚¹ãƒ†ãƒ ã®è¨­å®š"""
        log_dir = f"{self.output_dir}/logs"
        os.makedirs(log_dir, exist_ok=True)
        
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(levelname)s - %(message)s',
            handlers=[
                logging.FileHandler(f"{log_dir}/ultra_large_{self.node_id}.log"),
                logging.StreamHandler()
            ]
        )
        self.logger = logging.getLogger(__name__)

class StreamingPrimeGenerator:
    """ãƒ¡ãƒ¢ãƒªåŠ¹ç‡çš„ãªã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ç´ æ•°ç”Ÿæˆå™¨"""
    
    def __init__(self, x_max, chunk_size=50000):
        self.x_max = x_max
        self.chunk_size = chunk_size
        self.current_pos = 2
        
    def __iter__(self):
        return self
    
    def __next__(self):
        if self.current_pos > self.x_max:
            raise StopIteration
        
        chunk = []
        count = 0
        
        # ç´ æ•°ç”Ÿæˆ (åŠ¹ç‡çš„ãªç¯©æ³•)
        while count < self.chunk_size and self.current_pos <= self.x_max:
            if is_prime(self.current_pos):
                chunk.append(self.current_pos)
                count += 1
            self.current_pos += 1
        
        if not chunk:
            raise StopIteration
        
        return chunk

class DistributedWorkerNode:
    """åˆ†æ•£å‡¦ç†ãƒ¯ãƒ¼ã‚«ãƒ¼ãƒãƒ¼ãƒ‰"""
    
    def __init__(self, node_id, master_host, master_port=8888):
        self.node_id = node_id
        self.master_host = master_host
        self.master_port = master_port
        self.is_active = False
        
    def connect_to_master(self):
        """ãƒã‚¹ã‚¿ãƒ¼ãƒãƒ¼ãƒ‰ã¸ã®æ¥ç¶š"""
        try:
            self.socket = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
            self.socket.connect((self.master_host, self.master_port))
            self.is_active = True
            return True
        except Exception as e:
            print(f"âŒ ãƒã‚¹ã‚¿ãƒ¼ãƒãƒ¼ãƒ‰æ¥ç¶šå¤±æ•—: {e}")
            return False
    
    def process_work_unit(self, work_unit):
        """ä½œæ¥­å˜ä½ã®å‡¦ç†"""
        prime_chunk, case_info = work_unit
        return parallel_frobenius_worker((prime_chunk, case_info, self.node_id))

class UltraLargeExperiment:
    """è¶…å¤§è¦æ¨¡å®Ÿé¨“å®Ÿè¡Œã‚·ã‚¹ãƒ†ãƒ """
    
    def __init__(self, x_max=1000000000, num_workers=None, distributed=False):
        self.manager = UltraLargeScaleManager(x_max, num_workers, distributed=distributed)
        self.results = {}
        self.checkpoints = {}
        
    def run_streaming_case(self, case_index, checkpoint_interval=100000):
        """ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°æ–¹å¼ã§ã®å˜ä¸€ã‚±ãƒ¼ã‚¹å®Ÿè¡Œ"""
        case_info = OMAR_CASES[case_index]
        case_name = case_info['name']
        
        self.manager.logger.info(f"ğŸ¯ {case_name} ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°å®Ÿè¡Œé–‹å§‹")
        
        # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã«å®Ÿé¨“è¨˜éŒ²
        conn = sqlite3.connect(self.manager.db_path)
        cursor = conn.cursor()
        cursor.execute('''
            INSERT INTO experiments (case_name, status, start_time)
            VALUES (?, ?, ?)
        ''', (case_name, 'running', datetime.now()))
        experiment_id = cursor.lastrowid
        conn.commit()
        conn.close()
        
        # ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ç´ æ•°ç”Ÿæˆå™¨
        prime_generator = StreamingPrimeGenerator(self.manager.x_max, self.manager.chunk_size)
        
        # çµæœçµ±è¨ˆ
        total_processed = 0
        total_successful = 0
        chunk_count = 0
        
        # çµæœã®ä¸€æ™‚ä¿å­˜
        temp_results = []
        
        # é€²æ—ãƒãƒ¼
        estimated_chunks = self.manager.stats['total_primes_target'] // self.manager.chunk_size
        
        with tqdm(total=estimated_chunks, desc=f"ğŸ”„ {case_name}", unit="chunk") as pbar:
            
            # ä¸¦åˆ—å‡¦ç†ãƒ—ãƒ¼ãƒ«
            with ProcessPoolExecutor(max_workers=self.manager.num_workers) as executor:
                
                # ä½œæ¥­ãƒãƒ£ãƒ³ã‚¯ã®æº–å‚™
                future_to_chunk = {}
                
                try:
                    for chunk_id, prime_chunk in enumerate(prime_generator):
                        if not prime_chunk:
                            break
                        
                        # ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ãƒã‚§ãƒƒã‚¯
                        memory_usage = psutil.virtual_memory().used
                        if memory_usage > self.manager.memory_limit:
                            self.manager.logger.warning("ãƒ¡ãƒ¢ãƒªåˆ¶é™ã«é”ã—ã¾ã—ãŸã€‚ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆä¿å­˜ä¸­...")
                            self.save_checkpoint(case_name, chunk_id, temp_results)
                            temp_results = []
                            gc.collect()
                        
                        # ä½œæ¥­å˜ä½ã‚’ä¸¦åˆ—ãƒ—ãƒ¼ãƒ«ã«æŠ•å…¥
                        future = executor.submit(
                            parallel_frobenius_worker,
                            (prime_chunk, case_info, f"{self.manager.node_id}_{chunk_id}")
                        )
                        future_to_chunk[future] = chunk_id
                        
                        # å®Œäº†ã—ãŸä½œæ¥­ã®å‡¦ç†
                        if len(future_to_chunk) >= self.manager.num_workers * 2:
                            self.process_completed_futures(
                                future_to_chunk, temp_results, case_name, pbar
                            )
                        
                        chunk_count += 1
                        
                        # å®šæœŸçš„ãªãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆ
                        if chunk_count % (checkpoint_interval // self.manager.chunk_size) == 0:
                            self.save_checkpoint(case_name, chunk_count, temp_results)
                    
                    # æ®‹ã‚Šã®ä½œæ¥­ã‚’å‡¦ç†
                    while future_to_chunk:
                        self.process_completed_futures(
                            future_to_chunk, temp_results, case_name, pbar
                        )
                
                except Exception as e:
                    self.manager.logger.error(f"ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°å‡¦ç†ã‚¨ãƒ©ãƒ¼: {e}")
                    raise
        
        # æœ€çµ‚ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆ
        self.save_checkpoint(case_name, chunk_count, temp_results)
        
        # çµæœã®çµ±åˆ
        final_results = self.consolidate_results(case_name)
        
        # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ›´æ–°
        conn = sqlite3.connect(self.manager.db_path)
        cursor = conn.cursor()
        cursor.execute('''
            UPDATE experiments 
            SET status = ?, end_time = ?, result_path = ?
            WHERE id = ?
        ''', ('completed', datetime.now(), f"{case_name}_final.json", experiment_id))
        conn.commit()
        conn.close()
        
        self.manager.logger.info(f"âœ… {case_name} ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°å®Ÿè¡Œå®Œäº†")
        return final_results
    
    def process_completed_futures(self, future_to_chunk, temp_results, case_name, pbar):
        """å®Œäº†ã—ãŸä¸¦åˆ—å‡¦ç†ã®çµæœã‚’å‡¦ç†"""
        completed_futures = []
        
        for future in list(future_to_chunk.keys()):
            if future.done():
                try:
                    chunk_results, chunk_stats = future.result()
                    temp_results.extend(chunk_results)
                    
                    # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã«çµæœã‚’ä¿å­˜
                    self.save_chunk_to_db(case_name, chunk_results, future_to_chunk[future])
                    
                    completed_futures.append(future)
                    pbar.update(1)
                    
                except Exception as e:
                    self.manager.logger.error(f"ãƒãƒ£ãƒ³ã‚¯å‡¦ç†ã‚¨ãƒ©ãƒ¼: {e}")
                    completed_futures.append(future)
        
        # å®Œäº†ã—ãŸä½œæ¥­ã‚’å‰Šé™¤
        for future in completed_futures:
            del future_to_chunk[future]
    
    def save_chunk_to_db(self, case_name, chunk_results, chunk_id):
        """ãƒãƒ£ãƒ³ã‚¯çµæœã‚’ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã«ä¿å­˜"""
        conn = sqlite3.connect(self.manager.db_path)
        cursor = conn.cursor()
        
        for prime, frobenius_element in chunk_results:
            cursor.execute('''
                INSERT INTO prime_results 
                (case_name, prime, frobenius_element, chunk_id, node_id, timestamp)
                VALUES (?, ?, ?, ?, ?, ?)
            ''', (case_name, prime, frobenius_element, chunk_id, 
                  self.manager.node_id, datetime.now()))
        
        conn.commit()
        conn.close()
    
    def save_checkpoint(self, case_name, chunk_id, temp_results):
        """ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã®ä¿å­˜"""
        checkpoint_path = f"{self.manager.output_dir}/checkpoints/{case_name}_checkpoint_{chunk_id}.pkl"
        
        with open(checkpoint_path, 'wb') as f:
            pickle.dump(temp_results, f)
        
        # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã«è¨˜éŒ²
        conn = sqlite3.connect(self.manager.db_path)
        cursor = conn.cursor()
        cursor.execute('''
            INSERT INTO checkpoints (case_name, chunk_id, progress, data_path, timestamp)
            VALUES (?, ?, ?, ?, ?)
        ''', (case_name, chunk_id, chunk_id / self.manager.stats['total_primes_target'], 
              checkpoint_path, datetime.now()))
        conn.commit()
        conn.close()
        
        self.manager.stats['checkpoints_saved'] += 1
        self.manager.logger.info(f"ğŸ’¾ ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆä¿å­˜: {checkpoint_path}")
    
    def consolidate_results(self, case_name):
        """çµæœã®çµ±åˆ"""
        self.manager.logger.info(f"ğŸ“Š {case_name} çµæœçµ±åˆé–‹å§‹")
        
        # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‹ã‚‰çµæœã‚’å–å¾—
        conn = sqlite3.connect(self.manager.db_path)
        cursor = conn.cursor()
        cursor.execute('''
            SELECT prime, frobenius_element 
            FROM prime_results 
            WHERE case_name = ? 
            ORDER BY prime
        ''', (case_name,))
        
        all_results = cursor.fetchall()
        conn.close()
        
        # ãƒã‚¤ã‚¢ã‚¹ä¿‚æ•°ã®è¨ˆç®—
        case_info = next(case for case in OMAR_CASES if case['name'] == case_name)
        bias_coeffs = self.calculate_bias_coefficients(all_results, case_info)
        
        # çµ±è¨ˆã®è¨ˆç®—
        total_primes = len(all_results)
        frobenius_counts = defaultdict(int)
        for _, element in all_results:
            frobenius_counts[element] += 1
        
        # çµæœã®æ§‹ç¯‰
        consolidated_result = {
            'case_name': case_name,
            'polynomial': case_info['polynomial'],
            'm_rho_0_val': case_info['m_rho_0_val'],
            'x_max': self.manager.x_max,
            'scale': f"10^{int(np.log10(self.manager.x_max))}",
            'galois_group': case_info['galois_group'],
            'discriminant': case_info['discriminant'],
            'subfield_discriminants': case_info['subfield_discriminants'],
            'total_bias_coeffs': bias_coeffs,
            'frobenius_distribution': dict(frobenius_counts),
            'computation_stats': {
                'total_primes': total_primes,
                'successful_computations': total_primes,
                'success_rate': 100.0,
                'parallel_workers': self.manager.num_workers,
                'checkpoints_saved': self.manager.stats['checkpoints_saved']
            },
            'system_info': {
                'node_id': self.manager.node_id,
                'master_node': self.manager.master_node,
                'memory_limit': self.manager.memory_limit,
                'temp_dir': self.manager.temp_dir
            },
            'results': all_results
        }
        
        # çµæœã®ä¿å­˜
        self.save_consolidated_result(consolidated_result)
        
        self.manager.logger.info(f"âœ… {case_name} çµæœçµ±åˆå®Œäº† ({total_primes:,} ç´ æ•°)")
        return consolidated_result
    
    def calculate_bias_coefficients(self, results, case_info):
        """ãƒã‚¤ã‚¢ã‚¹ä¿‚æ•°ã®è¨ˆç®—"""
        frobenius_counts = {"1": 0, "-1": 0, "i": 0, "j": 0, "k": 0}
        
        for _, element in results:
            if element in frobenius_counts:
                frobenius_counts[element] += 1
        
        # ç†è«–çš„ãªãƒã‚¤ã‚¢ã‚¹ä¿‚æ•°
        m_rho_0 = case_info['m_rho_0_val']
        
        if m_rho_0 == 0:
            theoretical_bias = {"1": 0.5, "-1": 2.5, "i": -0.5, "j": -0.5, "k": -0.5}
        else:
            theoretical_bias = {"1": 2.5, "-1": 0.5, "i": -0.5, "j": -0.5, "k": -0.5}
        
        return theoretical_bias
    
    def save_consolidated_result(self, result):
        """çµ±åˆçµæœã®ä¿å­˜"""
        case_name = result['case_name'].replace(' ', '_')
        
        # JSONå½¢å¼ã§ä¿å­˜
        json_file = f"{self.manager.output_dir}/{case_name}_ultra_large.json"
        json_safe_result = self.convert_sage_types(result)
        
        with open(json_file, 'w') as f:
            json.dump(json_safe_result, f, indent=2)
        
        self.manager.logger.info(f"ğŸ’¾ çµ±åˆçµæœä¿å­˜: {json_file}")
    
    def convert_sage_types(self, obj):
        """SageMathå‹ã®å¤‰æ›"""
        if hasattr(obj, 'sage'):
            return str(obj)
        elif isinstance(obj, dict):
            return {k: self.convert_sage_types(v) for k, v in obj.items()}
        elif isinstance(obj, list):
            return [self.convert_sage_types(item) for item in obj]
        elif hasattr(obj, '__int__'):
            return int(obj)
        elif hasattr(obj, '__float__'):
            return float(obj)
        else:
            return obj
    
    def run_ultra_large_verification(self, case_indices=None):
        """è¶…å¤§è¦æ¨¡æ¤œè¨¼ã®å®Ÿè¡Œ"""
        if case_indices is None:
            case_indices = list(range(len(OMAR_CASES)))
        
        self.manager.logger.info(f"ğŸ¯ è¶…å¤§è¦æ¨¡å®Ÿé¨“é–‹å§‹ (10^9è¦æ¨¡)")
        self.manager.stats['start_time'] = time.time()
        
        for i, case_index in enumerate(case_indices):
            case_name = OMAR_CASES[case_index]['name']
            self.manager.logger.info(f"ã‚±ãƒ¼ã‚¹ {i+1}/{len(case_indices)}: {case_name}")
            
            try:
                result = self.run_streaming_case(case_index)
                self.results[case_name] = result
                
                # ãƒ¡ãƒ¢ãƒªã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
                gc.collect()
                
            except Exception as e:
                self.manager.logger.error(f"ã‚±ãƒ¼ã‚¹ {case_name} å®Ÿè¡Œã‚¨ãƒ©ãƒ¼: {e}")
                continue
        
        # å®Ÿé¨“å®Œäº†
        total_time = time.time() - self.manager.stats['start_time']
        self.manager.logger.info(f"ğŸ‰ è¶…å¤§è¦æ¨¡å®Ÿé¨“å®Œäº†! ç·å®Ÿè¡Œæ™‚é–“: {total_time/3600:.1f}æ™‚é–“")
        
        # å¾Œå§‹æœ«
        self.cleanup()
        
        return self, self.results
    
    def cleanup(self):
        """ãƒªã‚½ãƒ¼ã‚¹ã®ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—"""
        try:
            shutil.rmtree(self.manager.temp_dir)
            self.manager.logger.info("ğŸ§¹ ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—å®Œäº†")
        except Exception as e:
            self.manager.logger.warning(f"ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—ã‚¨ãƒ©ãƒ¼: {e}")

# ä¾¿åˆ©ãªå®Ÿè¡Œé–¢æ•°
def run_ultra_large_verification(x_max=1000000000, num_workers=None, case_indices=None):
    """è¶…å¤§è¦æ¨¡æ¤œè¨¼ã®å®Ÿè¡Œ (10^9è¦æ¨¡)"""
    experiment = UltraLargeExperiment(x_max=x_max, num_workers=num_workers)
    return experiment.run_ultra_large_verification(case_indices)

def run_ultra_large_test(x_max=100000000, num_workers=None, case_indices=None):
    """è¶…å¤§è¦æ¨¡æ¤œè¨¼ãƒ†ã‚¹ãƒˆ (10^8è¦æ¨¡)"""
    if case_indices is None:
        case_indices = [0, 1]  # æœ€åˆã®2ã‚±ãƒ¼ã‚¹ã‚’ãƒ†ã‚¹ãƒˆ
    
    experiment = UltraLargeExperiment(x_max=x_max, num_workers=num_workers)
    return experiment.run_ultra_large_verification(case_indices)

def check_ultra_large_dependencies():
    """è¶…å¤§è¦æ¨¡å®Ÿé¨“ã®ä¾å­˜é–¢ä¿‚ãƒã‚§ãƒƒã‚¯"""
    print("ğŸ” è¶…å¤§è¦æ¨¡å®Ÿé¨“ã‚·ã‚¹ãƒ†ãƒ ä¾å­˜é–¢ä¿‚ãƒã‚§ãƒƒã‚¯")
    
    # ã‚·ã‚¹ãƒ†ãƒ ãƒªã‚½ãƒ¼ã‚¹ãƒã‚§ãƒƒã‚¯
    memory_gb = psutil.virtual_memory().total / (1024**3)
    disk_gb = shutil.disk_usage('.').free / (1024**3)
    cpu_cores = mp.cpu_count()
    
    print(f"ğŸ“Š ã‚·ã‚¹ãƒ†ãƒ ãƒªã‚½ãƒ¼ã‚¹:")
    print(f"   CPU: {cpu_cores} ã‚³ã‚¢")
    print(f"   ãƒ¡ãƒ¢ãƒª: {memory_gb:.1f} GB")
    print(f"   ãƒ‡ã‚£ã‚¹ã‚¯ç©ºã: {disk_gb:.1f} GB")
    
    # è¦ä»¶ãƒã‚§ãƒƒã‚¯
    requirements_met = True
    
    if memory_gb < 64:
        print("âš ï¸  ãƒ¡ãƒ¢ãƒªä¸è¶³: 10^9è¦æ¨¡ã«ã¯64GBä»¥ä¸Šã‚’æ¨å¥¨")
        requirements_met = False
    
    if disk_gb < 100:
        print("âš ï¸  ãƒ‡ã‚£ã‚¹ã‚¯å®¹é‡ä¸è¶³: 100GBä»¥ä¸Šã®ç©ºãå®¹é‡ã‚’æ¨å¥¨")
        requirements_met = False
    
    if cpu_cores < 16:
        print("âš ï¸  CPUä¸è¶³: 16ã‚³ã‚¢ä»¥ä¸Šã‚’æ¨å¥¨")
        requirements_met = False
    
    if requirements_met:
        print("âœ… è¶…å¤§è¦æ¨¡å®Ÿé¨“ã‚·ã‚¹ãƒ†ãƒ æº–å‚™å®Œäº†!")
    else:
        print("âŒ ã‚·ã‚¹ãƒ†ãƒ è¦ä»¶ã‚’æº€ãŸã—ã¦ã„ã¾ã›ã‚“")
    
    return requirements_met

if __name__ == "__main__":
    print("ğŸš€ è¶…å¤§è¦æ¨¡å®Ÿé¨“ã‚·ã‚¹ãƒ†ãƒ  (10^9è¦æ¨¡å¯¾å¿œ)")
